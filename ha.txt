HA, load balancing and replication, backup and restore:

* Backup strategies: - SQL dump and restore

	- File system level backup

	- Continuous archiving

* File system level backup
	- tar up the data dir?
		-- worse than SQL dump. why?
			server needs to be brought down (buffering in the server and also the server
			needs to be shut down if we want to restore)

		-- lose capability of doing more granular backup/restore..pg_wal has no
		-- filtering capacity. So this method will only work for a full backup.

	- Consistent snapshot of the data dir.
		0. (Optional) CHECKPOINT;

		1. Use file-system feature to make "frozen snapshot" of the volume
		containing the database.

		2. Then copy over the datadir from that snapshot to a backup device.

		3. Release the frozen snapshot.

		-- No server downtime is required.

		-- Caveats: the datadir state captured like this will point to a state where
			the server was still up. If we try to bring the server back up in this
			situation, the server will think that it had crashed before and will go
			into recovery and try to replay WAL. If we did step 0, then recovery time
			will be lesser.

		-- If the setup has tablespaces, which are on different file systems, taking
			1. is difficult as simultaneous snapshot may be very difficult. In this case,
			this strategy IS NOT GOOD!!
			--- There is a workaround: shutdown the server and
			after a significant time period, take all of the frozen snapshots.
			OR 
			--- Enable continuous archiving only for the duration of 1-4. and then restore
			using continuous archive recovery.

	- Use rsync
		1. rsync while live

		2. Shutdown server

		3. rsync --checksum

* Continuous archiving and PITR:

	- Third strategy for backups: File-system
	level backup (base backup) + continuous backup of WAL files

	- Advantages: 
		-- We can start with a inconsistent file system as the starting
			point. We can just do with a tar, we don't need a consistent snapshot.
			--- Inconsistencies can be resolved after WAL replay.

		-- For large databases, where taking full backups is expensive, "continuous
			backup" can be achieved by continuously archiving the WAL files.

		-- It is not necessary to replay the WAL all the way, we could stop at any
			point and we will have a consistent snapshot of the database. It is
			possible to PITR to any point since the last base backup (since we have
			all the WAL since the last base backup).

			Instead of continuously archiving the WAL files, we could stream the WAL
			files over to another server that borne out of the same base backup. The
			result is a warm standby.

	- WAL archiving setup: -- WAL records are grouped into segment files (of
			16MB, but can be configured).

		-- Segment names are numerical and are recycled (only if we are not using
			WAL archiving).

		-- postgresql.conf 

			--- wal_level >= replica

			--- archive_mode = on

			--- archive_command = <shell command to copy over the WAL segments
				wherever> 
				> e.g. archive_command = 'test ! -f /mnt/server/archivedir/%f && cp
					%p /mnt/server/archivedir/%f' 
					(%p= <path_to_file>, %f = <file_name>).

				> e.g. substitution: 
				test ! -f /mnt/server/archivedir/00000001000000A900000065 
				&& cp pg_wal/00000001000000A900000065 /mnt/server/archivedir/00000001000000A900000065

		-- Setup has to be calibrated well enough to ensure the failure case for WAL
			archiving is handled well (what if archive command fails because of the
			destination? -> pg_wal will continue to build until the disk is full, in
			which case Postgres will perform a PANIC shutdown).

		-- It is important to have the right security for the WAL archive.

		-- One MUST keep in mind that restoring from a WAL archive will apply all
			possible changes EXCEPT CHANGES TO conf files such postgresql.conf!!!
			These changes ARE NOT WALed.

		-- The archive_command is invoked only when a WAL segment becomes full!
			That means that there can be a a long delay between the completion
			of a transaction and it's safe recording in archive storage.

			But wait..what if the server does not generate 16MB of WAL?? ->
			What we have to do then is configure archive_timeout to perform a
			"segment file switch" to a new file, making the old file a candidate
			for archiving.
			Now, the old file will still be sized at 16MB!!!!!! (this means that
			setting a small archive_timeout will lead to WAL bloat!!!!)

			Note: a WAL file switch can also be perfomed by explicitly calling
			pg_switch_wal().

			Also one may provide a script for the archive_command. For eg, one
			may want to batch WAL files together before sending them over.

		-- One can pause archival temporarily by overriding archive_command to ''.
			This will cause backup of wal files in pg_wal of course.

		-- archive_cleanup_command can be specified to periodically clean up WAL
			segments that may have aged. It can be anything. For a single standby
			mode, we can make do with pg_archivecleanup.


	- Taking a base backup
		-- Use tar mode or non-tar mode (tar -> extract to dir)

		-- Use binary or sql API + tar(needs lots of orchestration, but allows)
			for a lot more fine grained control.

		-- To make use of a base backup, one needs access to the WAL segments
			generated during and after the pg_basebackup invocation. To help us
			out, pg_basebackup generates a backup_history file that is named as
			follows: If the starting WAL file is 0000000100001234000055CD,
			history file name: 0000000100001234000055CD.007C9330.backup

			After we make copies of these, all WAL segments < than the name of
			the backup file can be deleted.

		-- Each backup has a label to uniquely identify it.

	- Recovering using a continuous archive backup [can be used to perform
		a targeted or continuous recovery (standby mode)]:
		Steps for a targeted recovery:

		1. Stop the server if its running

		2. At least make a copy of the server's pg_wal dir. It may contain logs
			that have not been archived since the server went down.

		3. Clean datadir and under tablespace roots.

		4. Restore the files from backup (make sure the files have the right permissions)
			and make sure pg_tblspc contains the correct links.

		5. Nuke the new pg_wal dir (as it represents old wal of the server taken
			during the earlier base backup) and replace it with a new one with
			the correct permissions.

		6. Copy the files from step 2 to this new pg_wal dir.

		7. Setup recovery settings (GUCs such as restore_command and recovery
			target to indicate how to recover and how much to recover.. Typically,
			the restore command is a cp command with a special format..described
			below)
			Also, create recovery.signal in datadir.

		8. Prevent users from connecting to the database while the database is
			recovering by temporarily updating pg_hba.conf

		9. Start the server. The server will read through the archived WAL and
			then the unarchived WAL from 6. Upon completion, the server itself
			will remove recovery.conf and then normal database operation will
			commence. If the recovery is halted at any time or there are any
			crashes, the recovery process can simply be restarted (recovery
			progress is tracked via updates made by the server to pg_control).

		10. Sanity check contents of database. If check doesn't pass go to 1.

	- Notion of timelines (to support arbitrary PITR)

	- full_page_writes

	- WAL archives are often compressed to save space


* Primary/master - server having r/w capabilities

* Secondary/standby - server having only r capability - warm standby: server
can't answer queries until it is promoted - hot standby: server can answer r/o
queries

* Sync solution: Tx is not considered committed and changes not reflected on
* any server, untill all of the servers have applied the changes. No stanby
* can fall behind.

* Solution can not only be deployment level but also database level or even,
* table level.

* Log shipping standbys setup:

	- Primary operates in continuous archiving mode.

	- Replica operates in continuous recovery mode.

	- File based log shipping:
		-- MTU: 1 log segment (1 log file) ~ 16MB

	- Streaming replication:

	- Record based log shipping:
		-- MTU: 

	- Log shipping is async..there exists a window for loss, i.e. primary goes
		down and the unshipped WAL is lost. 
		-- Size of data loss window regulated by the archive_timeout GUC.

		-- Streaming replication is a technique to lower the window considerably.

	- Constraint: All servers should be running the same version of Postgres.


	- Standby server operation: 
		-- archive-based replication: can read wal directly from a WAL archive
			(GUC: restore_command) 
			--- restore command: shell command with special syntax to retrieve an
				archived wal segment. e.g. 'cp /mnt/server/archivedir/%f "%p"'
				// %p->dest dir, %f->file from archive

		-- or can read wal directly from the master over a TCP connection. //streaming replication

		-- standby will also play anything in its own pg_wal directory.
			There can be stuff in that directory, especially if the standby went down while there was
			stuff shipped from the master but not replayed yet.

		-- Standby order of ops:
			1. Restore all WAL available in archive location using "restore_command".

			2. Once 1. is done (restore_command will fail if there are no files left),
				restore any files from its pg_wal dir.

			3. Once 2. is done, and streaming replication is set up, standby will
				connect to the primary (using primary_conninfo) and will start
				streaming WAL from the last valid record
				it found in step 1. or step 2.

			4. If step 3. fails, go back to step 1.

			This loop can be broken if:
			i) Server stops.

			ii) Failover triggered by a trigger file or by "pg_ctl promote".

	- Master server prep: 
		-- We need to setup the master for continuous archiving as
			described before.

		-- The archive location MUST be accessible from the primary!!! (optional
			for a flavor of streaming replication)

		-- For streaming replication, pg_hba.conf and roles need to be setup to
			allow replication connections from the standby(s).

		-- max_wal_senders need to be high enough to support multiple standbys

	- Standby server prep:
		-- Restore the base backup of the primary.

		-- Create a standby.signal file in the standby's datadir

		-- Specify a restore_command to fetch WAL from the WAL archive.

		-- For multiple standbys, we have to set up the recovery_target_timeline.

		-- To use streaming replication, we must specify primary_conninfo.

		-- Set up continuous archiving as described above, for the rainy day when
			the standby will be promoted.

	- Streaming replication:
		-- Standby stays more up-to-date than with log-shipping.

		-- It is async by default!!! => lag b/w tx commit and changes on standby

		-- If used w/o file-based continuous archiving, the server might recycle
			old WAL segments before the standby gets a chance to receive them!
			
			There is the workaround by keeping "wal_keep_segments" high OR by
			configuring a replication slot for the standby.
			
			If the WAL archive is indeed accessible from the standby, then we don't
			need replication slots or the other workaround.

		-- Start with a file-based log shipping standby server and if we add
			the primary_conninfo GUC, and some auth stuff on the primary side, we
			end up with a streaming replication setup!!

		--  tcp_keepalives_idle, tcp_keepalives_interval and tcp_keepalives_count
			help the primary promptly notice a broken connection.

		-- Note: streaming replication will only start over the primary conn once
			the standby has replayed all outstanding WAL in the archive.

		-- Authentication setup....

		-- Monitoring:
			--- Replay lag and such can be calculated from pg_stat_replication.

			--- pg_current_lsn, sent_lsn, write_lsn, flush_lsn, replay_lsn etc.

	- Replication slots:
		-- Automation to ensure master does NOT remove any WAL that hasn't been
			sent to the standbys.

		-- Alternatives to replication slots:
			--- wal_keep_segments high enough.

			--- Continuous archival of WAL.

		-- Can lead to unbounded pg_wal sizes on the primary if replication slots
			are used, as opposed to the alternatives above.

		-- pg_replication_slots

		-- Cascading replication: master -> standby (C) -> standby.

			--- Minimzes connectivity to the master.

	- Sync rep for streaming replication:
		-- Allows for confirmation to a tx that all standbys have
			written/flushed/applied all WAL associated with the tx.

		-- synchronous_standby_names set and synchronous_commit = <write/apply...>
			synchronous_commit can be as granular as we want.

		-- May cause server shutdown to halt! (wal for an outstanding tx MUST be shipped)

		-- Multi-standby setup..setup can be elastic as well.

		-- Mode: streaming vs catchup
			--- When the standby connects to the primary for the first time, it
				will enter catchup mode and then it will transition over to
				streaming when it is caught up.



