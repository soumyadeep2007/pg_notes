* Parallel query plans will have a Gather or a Gather merge node.
	- Example:
	EXPLAIN SELECT * FROM pgbench_accounts WHERE filler LIKE '%x%';
	                                     QUERY PLAN                                      
	-------------------------------------------------------------------â€‹------------------
	 Gather  (cost=1000.00..217018.43 rows=1 width=97)
	   Workers Planned: 2
	   ->  Parallel Seq Scan on pgbench_accounts  (cost=0.00..216018.33 rows=1 width=97)
	         Filter: (filler ~~ '%x%'::text)
	(4 rows)

	- In all cases, Gather{Merge} will always have exactly 1 child plan, which
		will always be executed in parallel.

* Backend will request # workers = # workers planned.

* # workers planned <= max_parallel_workers_per_gather.

* Total # background workers that is granted is limited by max_worker_processes
and max_parallel_workers. (A "parallel worker" is a subset of "worker")

* It is very much possible that # workers granted < # workers planned.

* The leader, like the parallel workers, would also execute the part of the plan
	below the G/GM node. However it will also have to read all of the tuples sent
	by the workers. 

	Depending on the # tuples generated by the parallel workers, the amount of
	time the leader will spend on the execution of the parallel plan vs reading
	tuples will vary.

* For a GM node, the leader would have to additionally do an order-preserving
	merge.

* There can be a variety of reasons why the planner can't choose a parallel plan
	in a given circumstance. For eg, if the parallel plan will be under an
	already parallel plan, or if the query calls a function with PARALLEL UNSAFE.

* There can be a variety of reasons why the executor can't execute a plan in
	parallel even though it was planned to be so. (In such a case, a G{M} node
	would be a no-op). Eg. max_parallel_workers and max_worker_processes
	restrictions etc.

* The parallel portion of a plan is termed "partial". It is constructed in a way
	such that every worker would return a part of the final result set and the
	result sets would be mutually exclusive.

* Generally any parallel plan involves a parallel-aware scan on a driving table.

* Parallel scans:
	- Parallel seq scan: Table's blocks are divided among workers, one at a time
	so that access to the table remains sequential.

* Parallel joins:

* Parallel agg:

* Parallel append:

* Easy way to force parallelism is to set parallel_setup_cost and parallel_tuple_cost
	to 0.

* EXPLAIN (ANALYZE, VERBOSE) will display per-worker statistics.

* ParallelContext: Facility to write parallel algorithms in Postgres. With it:
	- Launch background processes.

	- Init their state w/ the same state as the backend that spawned the process

	- IPC w/ dynamic shared memory.

	- Code can be written such that it will be agnostic of whether it is running
		in the leader or the background worker.

	- Initiating backend creates a dynamic shared memory segment which will last
		the lifetime of the parallel operation.
			-- The segment contains a shm_mq that can be used to transfer errors
				by elog/ereport from the background process to the initiating
				backend.

			-- The segment will contain a serialized representation of the
				initiating backend's private state, so that the worker can sync
				its state w/ the initiating backend.

			-- Any custom data structures that the algorithm author would like
				to add.

	- Once the backend has created the segment, it asks the postmaster to launch
		the appropriate # parallel workers. Then the workers connect to the
		segment, init their state and then invoke the appropriate entry point.

	- Error reporting:
		-- Any error that occurs before the worker can attach to the segment
			can't be bubbled up through the shm_mq.

		-- Whenever a new message is sent to the shm_mq, PROCSIG_PARALLEL_MESSAGE
			is sent to the backend. This causes the next CHECK_FOR_INTERRUPTS()
			in the backend to read and rethrow the message.

	- State sharing:
		-- 

